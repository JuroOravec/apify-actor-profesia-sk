{
  "title": "Profesia.sk Scraper",
  "description": "Configure the Profesia.sk Scraper. NOTE: Either \"Dataset type\" or \"Start URLs\" must be given.",
  "type": "object",
  "schemaVersion": 1,
  "properties": {
    "datasetType": {
      "type": "string",
      "title": "Dataset type",
      "description": "Use this option if you want to scrape whole dataset, not just specific URLs. Choose what kind of data you want to extract - job offers, list of companies, list of industries, ... This option is ignored if 'startUrls' is given",
      "editor": "select",
      "example": "jobOffers",
      "default": "jobOffers",
      "prefill": "jobOffers",
      "enum": [
        "jobOffers",
        "industies",
        "positions",
        "companies",
        "locations",
        "languages",
        "partners"
      ],
      "enumTitles": [
        "Job offers",
        "Industries",
        "Positions",
        "Companies",
        "Locations",
        "Languages",
        "partners"
      ],
      "nullable": false
    },
    "startUrls": {
      "title": "Start URLs",
      "type": "array",
      "description": "Select specific URLs to scrape. This option takes precedence over the 'dataset type' input. If the URL is a listing page or a company page, all entries of that list are extracted. If the URL is a company page, only that page is extracted",
      "prefill": [
        {
          "url": "https://www.profesia.sk/praca"
        }
      ],
      "editor": "requestListSources"
    },
    "proxy": {
      "title": "Proxy configuration",
      "type": "object",
      "description": "Select proxies to be used by your crawler.",
      "editor": "proxy"
    },

    "jobOfferDetailed": {
      "title": "Detailed",
      "type": "boolean",
      "description": "If checked, the scraper will obtain more detailed info for job offers by visit the details page of each job offer to extract data. If un-checked, only the data from the listing page is extracted. For details, please refer to http://apify.com/store/jurooravec/profesia-sk-scraper#output",
      "example": true,
      "default": true,
      "sectionCaption": "Job Offer Filters",
      "sectionDescription": "These filters are applied ONLY when 'Dataset type' is set to 'Job offers' (Optional)",
      "nullable": true
    },
    "jobOfferFilterQuery": {
      "type": "string",
      "title": "Search query (full-text search)",
      "description": "If given, only entries matching the query will be retrieved (full-text search)",
      "example": "specialist",
      "editor": "textfield",
      "nullable": true
    },
    "jobOfferFilterMaxCount": {
      "title": "Max number of results",
      "type": "integer",
      "description": "If set, only up to this number of entries will be extracted. The actual number of entries might higher than this due to multiple pages being scraped at the same time.",
      "example": 10,
      "minimum": 1,
      "nullable": true
    },
    "jobOfferFilterMinSalaryValue": {
      "title": "Min salary",
      "type": "integer",
      "description": "If set, only entries offering this much or more will be extracted",
      "example": 10,
      "minimum": 1,
      "nullable": true
    },
    "jobOfferFilterMinSalaryPeriod": {
      "title": "Min salary per hour/month",
      "type": "string",
      "description": "Choose if the minimum salary is in per hour or per month format",
      "editor": "select",
      "example": "month",
      "default": "month",
      "prefill": "month",
      "enum": [
        "month",
        "hour"
      ],
      "enumTitles": [
        "Per month",
        "Per hour"
      ],
      "nullable": true
    },
    "jobOfferFilterEmploymentType": {
      "title": "Type of employment",
      "type": "string",
      "description": "If set, only entries with this employment filter will be extracted",
      "editor": "select",
      "example": "fte",
      "enum": [
        "fte",
        "pte",
        "selfemploy",
        "voluntary",
        "internship"
      ],
      "enumTitles": [
        "Full-time employment",
        "Part-time employment",
        "Freelance (Self-employed)",
        "Voluntary work",
        "Internship"
      ],
      "nullable": true
    },
    "jobOfferFilterRemoteWorkType": {
      "title": "Remote vs On-site",
      "type": "string",
      "description": "If set, only entries with this type of remote work filter will be extracted",
      "editor": "select",
      "example": "fullRemote",
      "enum": [
        "fullRemote",
        "partialRemote",
        "noRemote"
      ],
      "enumTitles": [
        "Full remote (Work from home)",
        "Partial remote",
        "On-site only"
      ],
      "nullable": true
    },
    "jobOfferFilterLastNDays": {
      "title": "Last N days",
      "type": "integer",
      "description": "If set, only entries this much days old will be extracted. E.g. 7 = 1 week old, 31 = 1 month old, ...",
      "example": 10,
      "minimum": 0,
      "nullable": true
    },
    "jobOfferCountOnly": {
      "title": "Count the matched job offers",
      "type": "boolean",
      "description": "If checked, no data is extracted. Instead, the count of matched job offers is printed in the log.",
      "default": false,
      "groupCaption": "Troubleshooting options",
      "groupDescription": "Use these to verify that your custom startUrls are correct",
      "nullable": true
    },

    "maxRequestRetries": {
      "title": "maxRequestRetries",
      "type": "integer",
      "description": "Indicates how many times the request is retried if BasicCrawlerOptions.requestHandler fails.",
      "example": 3,
      "prefill": 3,
      "minimum": 0,
      "nullable": true,
      "sectionCaption": "Crawler configuration (Advanced)",
      "sectionDescription": "These options are applied directly to the Crawler. In majority of cases you don't need to change these. See https://crawlee.dev/api/basic-crawler/interface/BasicCrawlerOptions"
    },
    "maxRequestsPerMinute": {
      "title": "maxRequestsPerMinute",
      "type": "integer",
      "description": "The maximum number of requests per minute the crawler should run. We can pass any positive, non-zero integer.",
      "example": 120,
      "prefill": 120,
      "minimum": 1,
      "nullable": true
    },
    "maxRequestsPerCrawl": {
      "title": "maxRequestsPerCrawl",
      "type": "integer",
      "description": "Maximum number of pages that the crawler will open. The crawl will stop when this limit is reached. <br/><br/> NOTE: In cases of parallel crawling, the actual number of pages visited might be slightly higher than this value.",
      "example": 120,
      "minimum": 1,
      "nullable": true
    },
    "minConcurrency": {
      "title": "minConcurrency",
      "type": "integer",
      "description": "Sets the minimum concurrency (parallelism) for the crawl.\n\nWARNING: If we set this value too high with respect to the available system memory and CPU, our crawler will run extremely slow or crash. If not sure, it's better to keep the default value and the concurrency will scale up automatically.",
      "example": 3,
      "prefill": 1,
      "minimum": 1,
      "nullable": true
    },
    "maxConcurrency": {
      "title": "maxConcurrency",
      "type": "integer",
      "description": "Sets the maximum concurrency (parallelism) for the crawl.",
      "example": 3,
      "minimum": 1,
      "nullable": true
    },
    "navigationTimeoutSecs": {
      "title": "navigationTimeoutSecs",
      "type": "integer",
      "description": "Timeout in which the HTTP request to the resource needs to finish, given in seconds.",
      "example": 60,
      "minimum": 0,
      "nullable": true
    },
    "requestHandlerTimeoutSecs": {
      "title": "requestHandlerTimeoutSecs",
      "type": "integer",
      "description": "Timeout in which the function passed as BasicCrawlerOptions.requestHandler needs to finish, in seconds.",
      "example": 60,
      "prefill": 180,
      "minimum": 0,
      "nullable": true
    },
    "keepAlive": {
      "title": "keepAlive",
      "type": "boolean",
      "description": "Allows to keep the crawler alive even if the RequestQueue gets empty. With keepAlive: true the crawler will keep running, waiting for more requests to come.",
      "nullable": true
    },
    "ignoreSslErrors": {
      "title": "ignoreSslErrors",
      "type": "boolean",
      "description": "If set to true, SSL certificate errors will be ignored.",
      "nullable": true
    },
    "additionalMimeTypes": {
      "title": "additionalMimeTypes",
      "type": "array",
      "description": "An array of MIME types you want the crawler to load and process. By default, only text/html and application/xhtml+xml MIME types are supported.",
      "editor": "stringList",
      "uniqueItems": true,
      "nullable": true
    },
    "suggestResponseEncoding": {
      "title": "suggestResponseEncoding",
      "type": "string",
      "description": "By default this crawler will extract correct encoding from the HTTP response headers. There are some websites which use invalid headers. Those are encoded using the UTF-8 encoding. If those sites actually use a different encoding, the response will be corrupted. You can use suggestResponseEncoding to fall back to a certain encoding, if you know that your target website uses it. To force a certain encoding, disregarding the response headers, use forceResponseEncoding.",
      "editor": "textfield",
      "nullable": true
    },
    "forceResponseEncoding": {
      "title": "forceResponseEncoding",
      "type": "string",
      "description": "By default this crawler will extract correct encoding from the HTTP response headers. Use forceResponseEncoding to force a certain encoding, disregarding the response headers. To only provide a default for missing encodings, use suggestResponseEncoding.",
      "editor": "textfield",
      "nullable": true
    }
  }
}
